@inproceedings{saxena-etal-2020-keygames,
    title = "{K}ey{G}ames: A Game Theoretic Approach to Automatic Keyphrase Extraction",
    author = "Saxena, Arnav  and
      Mangal, Mudit  and
      Jain, Goonjan",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.184",
    doi = "10.18653/v1/2020.coling-main.184",
    pages = "2037--2048",
    abstract = "In this paper, we introduce two advancements in the automatic keyphrase extraction (AKE) space - KeyGames and pke+. KeyGames is an unsupervised AKE framework that employs the concept of evolutionary game theory and consistent labelling problem to ensure consistent classification of candidates into keyphrase and non-keyphrase. Pke+ is a python based pipeline built on top of the existing pke library to standardize various AKE steps, namely candidate extraction and evaluation, to ensure truly systematic and comparable performance analysis of AKE models. In the experiments section, we compare the performance of KeyGames across three publicly available datasets (Inspec 2001, SemEval 2010, DUC 2001) against the results quoted by the existing state-of-the-art models as well as their performance when reproduced using pke+. The results show that KeyGames outperforms most of the state-of-the-art systems while generalizing better on input documents with different domains and length. Further, pke+{'}s pre-processing brings out improvement in several other system{'}s quoted performance as well.",
}

@article{https://doi.org/10.1111/bjet.13484,
author = {Mangal, Mudit and Pardos, Zachary A.},
title = {Implementing equitable and intersectionality-aware ML in education: A practical guide},
journal = {British Journal of Educational Technology},
volume = {55},
number = {5},
pages = {2003-2038},
keywords = {algorithmic fairness, educational decision support systems, equity framework, institutional values, intersectionality, ML in education},
doi = {https://doi.org/10.1111/bjet.13484},
url = {https://bera-journals.onlinelibrary.wiley.com/doi/abs/10.1111/bjet.13484},
eprint = {https://bera-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.13484},
abstract = {Abstract The greater the proliferation of AI in educational contexts, the more important it becomes to ensure that AI adheres to the equity and inclusion values of an educational system or institution. Given that modern AI is based on historic datasets, mitigating historic biases with respect to protected classes (ie, fairness) is an important component of this value alignment. Although extensive research has been done on AI fairness in education, there has been a lack of guidance for practitioners, which could enhance the practical uptake of these methods. In this work, we present a practitioner-oriented, step-by-step framework, based on findings from the field, to implement AI fairness techniques. We also present an empirical case study that applies this framework in the context of a grade prediction task using data from a large public university. Our novel findings from the case study and extended analyses underscore the importance of incorporating intersectionality (such as race and gender) as central equity and inclusion institution values. Moreover, our research demonstrates the effectiveness of bias mitigation techniques, like adversarial learning, in enhancing fairness, particularly for intersectional categories like race–gender and race–income. Practitioner notes What is already known about this topic AI-powered Educational Decision Support Systems (EDSS) are increasingly used in various educational contexts, such as course selection, admissions, scholarship allocation and identifying at-risk students. There are known challenges with AI in education, particularly around the reinforcement of existing biases, leading to unfair outcomes. The machine learning community has developed metrics and methods to measure and mitigate biases, which have been effectively applied to education as seen in the AI in education literature. What this paper adds Introduces a comprehensive technical framework for equity and inclusion, specifically for machine learning practitioners in AI education systems. Presents a novel modification to the ABROCA fairness metric to better represent disparities among multiple subgroups within a protected class. Empirical analysis of the effectiveness of bias-mitigating techniques, like adversarial learning, in reducing biases in intersectional classes (eg, race–gender, race–income). Model reporting in the form of model cards that can foster transparent communication among developers, users and stakeholders. Implications for practice and/or policy The fairness framework can act as a systematic guide for practitioners to design equitable and inclusive AI-EDSS. The fairness framework can act as a systematic guide for practitioners to make compliance with emerging AI regulations more manageable. Stakeholders may become more involved in tailoring the fairness and equity model tuning process to align with their values.},
year = {2024}
}


